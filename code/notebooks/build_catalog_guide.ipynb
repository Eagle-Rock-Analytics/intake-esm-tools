{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š How to Build an Intake-ESM Catalog  \n",
    "\n",
    "**Author:** Nicole Keeney  \n",
    "**Creation Date:** March 2025  \n",
    "**Last Modified:** N/A  \n",
    "\n",
    "## ðŸ“– Overview  \n",
    "This notebook provides a step-by-step guide to building an **Intake-ESM** catalog for zarrs in an s3 bucket, including how to use `ecgtools` to create a custom parser and structure datasets for use with `intake-esm`.   \n",
    "\n",
    "This notebook is divided into four steps: \n",
    "1) Build a custom parser function \n",
    "2) Build the catalog object \n",
    "3) Export the catalog files\n",
    "4) Read in some files and verify that it all worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs \n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import intake \n",
    "from ecgtools import Builder\n",
    "from ecgtools.builder import INVALID_ASSET, TRACEBACK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build a custom parser function \n",
    "This function extracts information from the filepath, such that it can be used by the `Builder` to generate the function. For the input filepaths, the path to the zarr will include the `.zmetadata` extension, even though the `path` key in the output dictionary will **not** include this file extension; I realize this is confusing, but it's a hacky way to get around some inflexibility in the `Builder` class when working with zarrs. See the section above for more info: **Additional notes on the inputs to Builder: include_patterns**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ae_ren_data(filepath):\n",
    "    \"\"\"\n",
    "    Parses the S3 filepath to extract metadata for climate simulation data.\n",
    "    \n",
    "    Extracts information like installation, simulation model, experiment, \n",
    "    frequency, variable, grid resolution, and the file path (without `.zmetadata`).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        The S3 URL of the file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with parsed metadata:\n",
    "        - installation, activity_id, institution_id, source_id, experiment_id, \n",
    "          table_id, variable_id, grid_label, path.\n",
    "        If parsing fails, returns a dictionary with the error details:\n",
    "        - INVALID_ASSET and TRACEBACK.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> parse_ae_ren_data('s3://wfclimres/ERA/WRF/EC-Earth3/experiment/precipitation/variable/zarr/file.zmetadata')\n",
    "    {\n",
    "        \"installation\": \"WRF\",\n",
    "        \"activity_id\": \"WRF\",\n",
    "        \"institution_id\": \"ERA\",\n",
    "        \"source_id\": \"EC-Earth3\",\n",
    "        \"experiment_id\": \"experiment\",\n",
    "        \"table_id\": \"precipitation\",\n",
    "        \"variable_id\": \"variable\",\n",
    "        \"grid_label\": \"zarr\",\n",
    "        \"path\": \"s3://wfclimres/ERA/WRF/EC-Earth3/experiment/precipitation/variable/zarr/file\"\n",
    "    }\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The `try/except` block handles errors in extracting information from the `filepath`. \n",
    "    If the filepath structure does not match the expected format or if any error occurs \n",
    "    while splitting the string, the `except` block will capture the exception and return \n",
    "    a dictionary with the error message and traceback.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the data info from the filepath\n",
    "        institution_id, installation, source_id, experiment_id, table_id, variable_id, grid_label, _ = filepath.split(\"s3://wfclimres/\")[1].split(\"/\")\n",
    "        # Remove .zmetadata from the filepath, since the actual path to the zarr doesn't include this \n",
    "        filepath = filepath.split(\".zmetadata\")[0]\n",
    "    except Exception as e:\n",
    "        # If an error occurs (e.g., wrong filepath structure), return error details\n",
    "        return {INVALID_ASSET: filepath, TRACEBACK: traceback.format_exc()}\n",
    "    \n",
    "    # Simulation string mapping\n",
    "    simulation_dict = {\n",
    "        \"ec-earth3\": \"EC-Earth3\",\n",
    "        \"mpi-esm1-2-hr\": \"MPI-ESM1-2-HR\",\n",
    "        \"miroc6\": \"MIROC6\",\n",
    "        \"taiesm1\": \"TaiESM1\",\n",
    "        \"era5\": \"ERA5\"\n",
    "    }\n",
    "\n",
    "    # Add filepath info to dictionary\n",
    "    info = {\n",
    "        \"installation\": installation,\n",
    "        \"activity_id\": \"WRF\", \n",
    "        \"institution_id\": \"ERA\",\n",
    "        \"source_id\": simulation_dict[source_id],\n",
    "        \"experiment_id\": experiment_id,\n",
    "        \"table_id\": table_id,\n",
    "        \"variable_id\": variable_id,\n",
    "        \"grid_label\": grid_label,\n",
    "        \"path\": filepath\n",
    "    }\n",
    "    \n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build the catalog object\n",
    "Using the custom parser, we will create our Builder object and build the catalog. How this is coded up will depend on various inputs. I'll show some different methods below.<br><br>\n",
    "Each method will use some variation of the following code: \n",
    "```python \n",
    "# Base Builder object \n",
    "b = Builder(paths=[\"s3://path-to-data-directory\"]) \n",
    "\n",
    "# Build the catalog using a custom parsing function (you need to define this function for your unique data structure)\n",
    "b.build(parsing_func=custom_parsing_func)\n",
    "\n",
    "# Exclude invalid assets and removing duplicate entries\n",
    "b.clean_dataframe()\n",
    "\n",
    "# View your build catalog as a dataframe :) \n",
    "b.df\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Feed the Builder allllll the filepaths (no crawling required)\n",
    "The Builder won't do any crawling of your data bucket, because you've oh so kindly fed it all the filepaths it needs. This method requires **you** to do the crawling beforehand to generate a list of these filepaths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I've written a bunch of code to crawl through the renewables s3 bucket and get filepaths for all the files I want to include in the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "# Use these to filter the s3 bucket \n",
    "installations = [\"pv_distributed\", \"pv_utility\", \"windpower_offshore\", \"windpower_onshore\"]\n",
    "source_ids = [\"ec-earth3\", \"miroc6\", \"mpi-esm1-2-hr\", \"taiesm1\", \"era5\"]\n",
    "\n",
    "# Total iterations for tqdm\n",
    "total_iterations = len(installations) * len(source_ids)\n",
    "\n",
    "filepaths = [] # Store all filepaths here \n",
    "with tqdm(total=total_iterations, desc=\"Scanning S3\", unit=\"query\") as pbar:\n",
    "    for installation in installations:\n",
    "        for source_id in source_ids:\n",
    "            # I think each unique zarr store has a single .zmetadata file associated with it \n",
    "            # Use .zmetadata to grab path to the main zarr store \n",
    "            # Otherwise you get all the random stuff associated with it (variables, coords, etc) since zarr is a directory, not a single file \n",
    "            glob_s3 = fs.glob(f\"s3://wfclimres/era/{installation}/{source_id}/**/*.zmetadata\")\n",
    "            zarr_paths = [\"s3://\"+file.split(\".zmetadata\")[0] for file in glob_s3] # Remove .zmetadata from the path \n",
    "            filepaths += zarr_paths \n",
    "            pbar.update(1)  # Update progress bar\n",
    "\n",
    "print(f\"Total files found: {len(filepaths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, feed these filepaths to the Builder. Since these filepaths are already **absolute filepaths**, set the argument ``depth=0``: no crawling required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = Builder(paths=filepaths, depth=0, include_patterns=[\"**/.zmetadata\"])\n",
    "b1.build(parsing_func=parse_ae_ren_data)\n",
    "b1.clean_dataframe()\n",
    "b1.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Feed the Builder a directory containing your files \n",
    "In this method, this Builder will do some of the hard work for you by crawling through your directories looking for files matching your specifications. Thanks, Builder!\n",
    "\n",
    "#### Notes on the \"depth\" Builder input\n",
    "In this case, the path to a file in our directory looks like this: `\"s3://wfclimres/era/pv_distributed/ec-earth3/historical/1hr/cf/d03/\"`<br>\n",
    "But, the path we are giving Builder looks like this: `\"s3://wfclimres/era/pv_distributed/\"`\n",
    "\n",
    "Thus, the `depth` for this Builder would  be `5`: We need to crawl through 5 different directories (`\"ec-earth3/historical/1hr/cf/d03/\"`) beyond the root directory to finally reach our zarr store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 's3://wfclimres/era/'\n",
    "installations = [\"pv_distributed\", \"pv_utility\", \"windpower_offshore\", \"windpower_onshore\"]\n",
    "b2 = Builder(\n",
    "    paths=[f's3://wfclimres/era/{installation}/' for installation in installations], \n",
    "    depth=5, \n",
    "    include_patterns=[\"**/.zmetadata\"]\n",
    ")\n",
    "b2.build(parsing_func=parse_ae_ren_data)\n",
    "b2.clean_dataframe()\n",
    "b2.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2b: Feed the Builder your entire bucket (slow and untested)\n",
    "In theory, you can also just feed the builder the entire bucket and have it crawl through everything. However, this method is really slow-- I haven't actually been patient enough to wait for the code to complete running, so I have it commented out below. I'm leaving it here for documentation's sake, in case it is useful for other s3 buckets in the future. \n",
    "\n",
    "#### Notes on the \"depth\" Builder input\n",
    "In this case, the path to a file in our directory looks like this: `\"s3://wfclimres/era/pv_distributed/ec-earth3/historical/1hr/cf/d03/\"`<br>\n",
    "But, the path we are giving Builder looks like this: `\"s3://wfclimres/era/\"`\n",
    "\n",
    "Thus, the `depth` for this Builder would  be `6`: We need to crawl through 6 different directories (`\"pv_distributed/ec-earth3/historical/1hr/cf/d03/\"`) beyond the root directory to finally reach our zarr store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_patterns = [\n",
    "#     \"s3://wfclimres/era/derived_products/**\",\n",
    "#     \"s3://wfclimres/era/resource_data/**\",\n",
    "#     \"s3://wfclimres/era/rsrc_drought/**\",\n",
    "#     \"s3://wfclimres/era/tmp/**\",\n",
    "#     \"s3://wfclimres/era/data-guide_pv-wind.pdf\"\n",
    "#     ]\n",
    "# b3 = Builder(\n",
    "#     paths=[\"s3://wfclimres/era/\"], \n",
    "#     depth=6, \n",
    "#     exclude_patterns=exclude_patterns, \n",
    "#     include_patterns=[\"**/.zmetadata\"]\n",
    "# )\n",
    "# b3.build(parsing_func=parse_ae_ren_data)\n",
    "# b3.clean_dataframe()\n",
    "# b3.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm that these methods are equal \n",
    "If everything went as expected, methods 1 and 2 should return the same result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reset the index on the second Builder since it's not ordered appropriately for some reason \n",
    "b1.df.equals(b2.df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Export the catalog files\n",
    "We need to export `csv` and `json` files associated with our built catalog. We also need to set the data aggregations which are fed into xarray when reading in data using intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2.save(\n",
    "    # Name to give the file \n",
    "    # Will create {name}.csv and {name}.json\n",
    "    name='era-ren-collection',\n",
    "    # Export to working directory\n",
    "    directory='', \n",
    "    # Column name including filepath\n",
    "    path_column_name='path',\n",
    "    # Column name including variables\n",
    "    variable_column_name='variable_id',\n",
    "    # Data file format - could be netcdf or zarr (in this case, zarr)\n",
    "    data_format=\"zarr\",\n",
    "    # Which attributes to groupby when reading in variables using intake-esm\n",
    "    groupby_attrs=[\"installation\",\"activity_id\",\"institution_id\",\"source_id\",\"experiment_id\",\"table_id\",\"grid_label\"], \n",
    "    # Aggregations which are fed into xarray when reading in data using intake\n",
    "    aggregations=[\n",
    "        {'type': 'union', 'attribute_name': 'variable_id'},\n",
    "    ],\n",
    "    description=\"Eagle Rock Analytics Renewables Data Catalog\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Read in some files and verify that it all worked! \n",
    "Read in the catalog and try to download some data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = intake.open_esm_datastore(\"era-ren-collection.json\")\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access catalog as dataframe and inspect the first few rows\n",
    "cat_df = cat.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form query dictionary\n",
    "query = {\n",
    "    # GCM name\n",
    "    'source_id': 'EC-Earth3',\n",
    "    # time period - historical or emissions scenario\n",
    "    'experiment_id': ['historical', 'ssp370'],\n",
    "    # variable\n",
    "    'variable_id': 'cf',\n",
    "    # time resolution \n",
    "    'table_id': 'day',\n",
    "    # grid resolution: d01 = 45km, d02 = 9km, d03 = 3km\n",
    "    'grid_label': 'd03'\n",
    "}\n",
    "\n",
    "# Subset catalog \n",
    "cat_subset = cat.search(**query)\n",
    "cat_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See dataframe from the catalog subset \n",
    "cat_subset.df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset dictionary \n",
    "dsets = cat_subset.to_dataset_dict(\n",
    "    xarray_open_kwargs={'consolidated': True},\n",
    "    storage_options={'anon': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display one of the files :) \n",
    "dsets[\"pv_distributed.WRF.ERA.EC-Earth3.historical.day.d03\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intake-esm-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
